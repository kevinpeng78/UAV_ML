{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Portions of the code in this work were adapted from the Google Cloud Platform Python documentation samples, available at: https://github.com/GoogleCloudPlatform/python-docs-samples, and are licensed under the Apache License, Version 2.0.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lgm7-iH2AJ86"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdTti_Sx244q"
      },
      "outputs": [],
      "source": [
        "import ee\n",
        "import folium\n",
        "ee.Authenticate()\n",
        "ee.Initialize(project='geowildfire')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import geemap.core as geemap\n",
        "\n",
        "\n",
        "start = '2020-01-01'\n",
        "end = '2020-12-31'\n",
        "naip = ee.ImageCollection('USDA/NAIP/DOQQ').filterDate(start, end)\n",
        "\n",
        "naip_crs = naip.first().projection().crs()\n",
        "naip_crs_transform = naip.first().projection().getInfo()['transform']\n",
        "naip = naip.median().reproject(crs=naip_crs, crsTransform=naip_crs_transform)\n",
        "\n",
        "#california geometry\n",
        "gaul = ee.FeatureCollection(\"FAO/GAUL/2015/level1\");\n",
        "california = gaul.filter(ee.Filter.eq('ADM1_NAME', 'California'));\n",
        "aoi = california.geometry()\n",
        "\n",
        "#add ndvi values\n",
        "ndvi = naip.normalizedDifference(['N', 'R']).rename('NDVI')\n",
        "naip = naip.addBands(ndvi)\n",
        "naip = naip.addBands(topo)\n",
        "naip = naip.addBands(landsat_temp)\n",
        "print(naip.getInfo())"
      ],
      "metadata": {
        "id": "9w_wsmkJ28BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "whp = ee.Image(\"projects/geowildfire/assets/whp2023\").select(0)\n",
        "whpPalette = [\n",
        "  '00a300', #very low\n",
        "  '2cff28', #low\n",
        "  'ffe100', #moderate\n",
        "  'fe6800', #high\n",
        "  'fe0017', #very high\n",
        "  '7a8f93', #nonburnable\n",
        "  '1017f4' #water\n",
        "\n",
        "\n",
        "]\n",
        "params = {\n",
        "    'min': 1,\n",
        "    'max': 7,\n",
        "    'palette': whpPalette\n",
        "}\n",
        "gmap = geemap.Map(center=[37.5010, -122.1899], zoom=10)\n",
        "points = whp.stratifiedSample(\n",
        "      numPoints=1000,\n",
        "      region=naip.geometry(),\n",
        "      scale=270,\n",
        "      geometries=True,\n",
        "  )\n",
        "print(naip.geometry())\n",
        "# Add the image layer to the map and display it.\n",
        "gmap.add_layer(whp, params, 'whp')\n",
        "display(gmap)\n"
      ],
      "metadata": {
        "id": "p_wwD3Fw3EhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Iterable, List, Tuple\n",
        "import numpy as np\n",
        "from google.api_core import exceptions, retry\n",
        "import requests\n",
        "import io\n",
        "def sample_random_points(region: ee.Geometry, points_per_class: int) -> Iterable[List]:\n",
        "  \"\"\"Get a generator of random points in the region.\"\"\"\n",
        "  points = whp.stratifiedSample(\n",
        "      points_per_class,\n",
        "      region=aoi,\n",
        "      scale=270,\n",
        "      geometries=True,\n",
        "  )\n",
        "  gmap.addLayer(points)\n",
        "  display(gmap)\n",
        "  for point in points.toList(points.size()).getInfo():\n",
        "      yield point['geometry']['coordinates']\n",
        "for point in sample_random_points(aoi, points_per_class=200):\n",
        "    print(point)\n",
        "\n"
      ],
      "metadata": {
        "id": "GnUk3urf3Ief"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@retry.Retry()\n",
        "def get_patch(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int) -> np.ndarray:\n",
        "\n",
        "  url = image.getDownloadURL({\n",
        "      'region': region,\n",
        "      'dimensions': [patch_size, patch_size],\n",
        "      'format': \"NPY\",\n",
        "      'bands': bands,\n",
        "  })\n",
        "\n",
        "  return np.load(io.BytesIO(response.content), allow_pickle=True)"
      ],
      "metadata": {
        "id": "NJYzM5Ys3KpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.recfunctions import structured_to_unstructured\n",
        "\n",
        "\n",
        "def get_input_patch(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int) -> np.ndarray:\n",
        "    patch = get_patch(naip, region, bands, patch_size)\n",
        "    return np.float32(structured_to_unstructured(patch))\n",
        "\n",
        "def get_label_patch(image: ee.Image, region: ee.Geometry, bands: List[str], patch_size: int) -> np.ndarray:\n",
        "    patch = get_patch(whp, region, bands, patch_size)\n",
        "    return structured_to_unstructured(patch)\n"
      ],
      "metadata": {
        "id": "bl17ah4I3MNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_training_example(\n",
        "    coords: List[float], patch_size: int = 450\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    point = ee.Geometry.Point(coords)\n",
        "    region = point.buffer(135, 1).bounds(1)\n",
        "    return (\n",
        "        get_input_patch(naip, region, ['NDVI'], patch_size),\n",
        "        get_label_patch(whp, region, ['b1'], patch_size/450),\n",
        "    )\n",
        "\n",
        "#-118.76193375968745, 37.867127218336286\n",
        "point = (-118.31690054334553, 36.897865747337185)  # (lon, lat)\n",
        "(inputs, labels) = get_training_example(point)\n",
        "print(f\"inputs : {inputs.dtype} {inputs.shape}\")\n",
        "print(f\"labels : {labels.dtype} {labels.shape}\")\n",
        "label_value = labels.item()\n",
        "print(f\"Label value: {label_value}\")\n",
        "print(np.mean(inputs))"
      ],
      "metadata": {
        "id": "5V6H-bo_3Nuz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def serialize(inputs: np.ndarray, labels: np.ndarray) -> bytes:\n",
        "    features = {\n",
        "        name: tf.train.Feature(\n",
        "            bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(data).numpy()])\n",
        "        )\n",
        "        for name, data in {\"inputs\": inputs, \"labels\": labels}.items()\n",
        "    }\n",
        "    example = tf.train.Example(features=tf.train.Features(feature=features))\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "serialized = serialize(inputs, labels)\n",
        "print(f\"serialized: {len(serialized)} bytes\")"
      ],
      "metadata": {
        "id": "iHvFSxyk3Wwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import apache_beam as beam\n",
        "from apache_beam.options.pipeline_options import PipelineOptions\n",
        "\n",
        "\n",
        "\n",
        "with beam.Pipeline() as pipeline:\n",
        "    (\n",
        "pipeline\n",
        "  | \"Create region\" >> beam.Create([aoi])\n",
        "  | \"Sample points\" >> beam.FlatMap(sample_random_points, points_per_class=50)\n",
        "  | \"Get examples\" >> beam.Map(get_training_example)\n",
        "  | \"Serialize\" >> beam.MapTuple(serialize)\n",
        "  | \"Write TFRecords\" >> beam.io.WriteToTFRecord(\"part\", file_name_suffix=\".tfrecord.gz\")\n",
        "    )"
      ],
      "metadata": {
        "id": "BlIL92vM3bUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"path/to/part-00000-of-00001.tfrecord.gz\""
      ],
      "metadata": {
        "id": "GYKI9p0f3dyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "filenames = tf.data.Dataset.list_files(path)\n",
        "dataset = tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\")\n",
        "\n",
        "for x in dataset.take(1):\n",
        "    print(f\"{type(x.numpy()).__name__} {len(x.numpy())}\")\n",
        "\n",
        "NUM_INPUTS = 1\n",
        "NUM_CLASSES = 7\n",
        "\n",
        "\n",
        "def read_example(serialized: bytes) -> tuple[tf.Tensor, tf.Tensor]:\n",
        "    features_dict = {\n",
        "        \"inputs\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"labels\": tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(serialized, features_dict)\n",
        "    inputs = tf.io.parse_tensor(example[\"inputs\"], tf.float32)\n",
        "    labels = tf.io.parse_tensor(example[\"labels\"], tf.uint8)\n",
        "\n",
        "    inputs.set_shape([450, 450, 1])\n",
        "    labels.set_shape([1, 1, 1])\n",
        "    labels -= 1\n",
        "    one_hot_labels = tf.one_hot(labels[:, :, 0], NUM_CLASSES)\n",
        "    return (inputs, tf.squeeze(one_hot_labels))\n",
        "\n",
        "\n",
        "filenames = tf.data.Dataset.list_files(path)\n",
        "dataset = tf.data.TFRecordDataset(filenames, compression_type=\"GZIP\").map(read_example)\n",
        "for inputs, labels in dataset.take(1):\n",
        "    print(f\"inputs : {inputs.dtype.name} {inputs.shape}\")\n",
        "    print(f\"labels : {labels.dtype.name} {labels.shape}\")\n",
        "\n",
        "for inputs, labels in dataset:\n",
        "    print(labels)\n"
      ],
      "metadata": {
        "id": "O3P0Bga036CT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "import keras\n",
        "\n",
        "input_shape = (270, 270, 1)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[450, 450, 1]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=2, strides=2))\n",
        "\n",
        "model.add(Conv2D(filters=16, kernel_size=3, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPool2D(pool_size=2, strides=2))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=1e-4),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"path/to/models\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\",\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "model.summary()\n",
        "history = model.fit(train_dataset, validation_data = test_dataset, epochs = 1000, callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "xFC4jSNV3-Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XdDG5RBp4Ubp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = tf.keras.models.load_model(\"path/to/mymodel_825\")\n",
        "best_model.summary()"
      ],
      "metadata": {
        "id": "F8sguqR44ZWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_files = tf.data.Dataset.list_files(path)\n",
        "test = tf.data.TFRecordDataset(test_files, compression_type=\"GZIP\").map(read_example)\n",
        "\n",
        "for inputs, labels in test:\n",
        "\n",
        "    print(labels)"
      ],
      "metadata": {
        "id": "z9g7Y3rM4nmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "y_true = []\n",
        "\n",
        "for inputs, labels in test:\n",
        "    y_true.append(np.argmax(labels, axis=-1))\n",
        "\n",
        "\n",
        "y_pred = (np.argmax(best_model.predict(test.batch(1)), axis=-1))\n",
        "\n",
        "\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "print(conf_matrix)\n"
      ],
      "metadata": {
        "id": "_GoYabIg4eQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Reds\", cbar=True,\n",
        "            xticklabels=['Very Low', 'Low', 'Moderate', 'High', 'Very High', 'Non-burnable', 'Water'],\n",
        "            yticklabels=['Very Low', 'Low', 'Moderate', 'High', 'Very High', 'Non-burnable', 'Water'], annot_kws={\"size\": 20})\n",
        "plt.title(\"Confusion Matrix\", fontsize=20)\n",
        "plt.xlabel(\"Predicted Class\", fontsize=18)\n",
        "plt.ylabel(\"True Class\", fontsize=18)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FQgmnuYE5JWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "report_dict = classification_report(y_true, y_pred, target_names=['Very Low', 'Low', 'Moderate', 'High', 'Very High', 'Non-burnable', 'Water'], output_dict=True)\n",
        "report_df = pd.DataFrame(report_dict).transpose()\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(report_df.iloc[:-3, :-1], annot=True, fmt=\".2f\", cmap=\"Reds\", annot_kws={\"size\": 20}, vmin=0, vmax=1)\n",
        "plt.title(\"Classification Report\")\n",
        "plt.title(\"Classification Report\", fontsize=20)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fjuurjOg5Ni2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}